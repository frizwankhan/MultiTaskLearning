
  0%|                                                                                                                                                                              | 0/248 [00:00<?, ?it/s]
memory occupied before training 1.5221610069274902
memory occupied before train_setp 1.5221610069274902
memory occupied after train_setp 1.6393485069274902
memory occupied  5.349981784820557
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 131072, 96])
memory occupied  8.673774242401123
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 131072, 96])
memory occupied  10.820383071899414
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 32768, 192])
memory occupied  12.497265338897705
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 32768, 192])
memory occupied  13.408890724182129
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  14.273396968841553
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  15.137903213500977
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  16.0024094581604
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  16.866915702819824
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  17.731421947479248
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  18.19670009613037
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 2048, 768])
memory occupied  18.637794017791748
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 2048, 768])
memory occupied  18.889881134033203
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 512, 1536])
memory occupied  19.083271503448486
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 512, 1536])
memory occupied  19.587361335754395
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 2048, 768])
memory occupied  19.974202632904053
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 2048, 768])
memory occupied  21.02962303161621
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  21.87386178970337
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  23.986166954040527
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 32768, 192])
memory occupied  25.675620555877686
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 32768, 192])
memory occupied  25.98621368408203
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 512, 1536])
memory occupied  26.179604053497314
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 512, 1536])
memory occupied  26.683693885803223
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 2048, 768])
memory occupied  27.07053518295288
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 2048, 768])
memory occupied  28.12595558166504
memory occupied  0.0
memory occupied  0.0
torch.Size([4, 8192, 384])
memory occupied  28.970194339752197
memory occupied  0.0
memory occupied  0.0
  0%|                                                                                                                                                                              | 0/248 [00:00<?, ?it/s]Traceback (most recent call last):
  File "main.py", line 68, in <module>
    trainer.train(config)
  File "/raid/home/rizwank/courses/DLCV/Project/train.py", line 115, in train
    loss, loss_dict = self.train_step(batch)
  File "/raid/home/rizwank/courses/DLCV/Project/train.py", line 44, in train_step
    output = self.model.module(images)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/model.py", line 206, in forward
    x = self.swin_encoder_decoder(x)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/model.py", line 120, in forward
    decoder_out, _ = self.task_decoder[task_id][i](decoder_inp)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/model.py", line 31, in forward
    out = self.model(x, output_hidden_states=True)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/transformers/src/transformers/models/swin/modeling_swin.py", line 1018, in forward
    encoder_outputs = self.encoder(
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/transformers/src/transformers/models/swin/modeling_swin.py", line 843, in forward
    layer_outputs = layer_module(
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/transformers/src/transformers/models/swin/modeling_swin.py", line 756, in forward
    layer_outputs = layer_module(
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/transformers/src/transformers/models/swin/modeling_swin.py", line 687, in forward
    attention_outputs = self.attention(
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/transformers/src/transformers/models/swin/modeling_swin.py", line 562, in forward
    self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/raid/home/rizwank/courses/DLCV/Project/transformers/src/transformers/models/swin/modeling_swin.py", line 475, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 111.38 MiB is free. Process 2991625 has 436.00 MiB memory in use. Including non-PyTorch memory, this process has 31.20 GiB memory in use. Of the allocated memory 30.05 GiB is allocated by PyTorch, and 594.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)